{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db46c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import requests\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3bc3d69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"https://olinda.bcb.gov.br/olinda/servico/PTAX/versao/v1/odata\"\n",
    "\n",
    "def _fmt_mmddyyyy(d: date) -> str:\n",
    "    return d.strftime(\"%m-%d-%Y\")\n",
    "\n",
    "def cotacao_dolar_periodo_df(data_ini: date, data_fim: date) -> pd.DataFrame:\n",
    "    url = f\"{BASE}/CotacaoDolarPeriodo(dataInicial=@dataInicial,dataFinalCotacao=@dataFinalCotacao)\"\n",
    "    params = {\n",
    "        \"@dataInicial\": f\"'{_fmt_mmddyyyy(data_ini)}'\",\n",
    "        \"@dataFinalCotacao\": f\"'{_fmt_mmddyyyy(data_fim)}'\",\n",
    "        \"$format\": \"json\",\n",
    "        \"$select\": \"cotacaoCompra,cotacaoVenda,dataHoraCotacao\",\n",
    "        \"$top\": 10000,\n",
    "    }\n",
    "\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    if not r.ok:\n",
    "        raise requests.HTTPError(f\"{r.status_code} - {r.text}\", response=r)\n",
    "\n",
    "    payload = r.json()\n",
    "    df = pd.DataFrame(payload.get(\"value\", []))\n",
    "\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df[\"dataHoraCotacao\"] = pd.to_datetime(df[\"dataHoraCotacao\"])\n",
    "    df = df.sort_values(\"dataHoraCotacao\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def dolar_diario(df_ptax: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Converte boletins intradiÃ¡rios em sÃ©rie diÃ¡ria (mÃ©dia por dia).\"\"\"\n",
    "    if df_ptax.empty:\n",
    "        return df_ptax\n",
    "\n",
    "    df = df_ptax.copy()\n",
    "    df[\"data\"] = df[\"dataHoraCotacao\"].dt.date\n",
    "    out = (\n",
    "        df.groupby(\"data\", as_index=False)[[\"cotacaoCompra\", \"cotacaoVenda\"]]\n",
    "          .mean()\n",
    "          .rename(columns={\"cotacaoCompra\": \"compra\", \"cotacaoVenda\": \"venda\"})\n",
    "    )\n",
    "    out[\"data\"] = pd.to_datetime(out[\"data\"])\n",
    "    return out.sort_values(\"data\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1a57029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_scrdata(\n",
    "    df: pd.DataFrame,\n",
    "    remover_zeros: bool = True,\n",
    "    criar_indicadores: bool = True,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processa automaticamente dados do SCR.data para anÃ¡lise e visualizaÃ§Ã£o.\n",
    "\n",
    "    Etapas:\n",
    "    - padroniza nomes das colunas\n",
    "    - converte datas\n",
    "    - converte valores monetÃ¡rios\n",
    "    - cria colunas temporais\n",
    "    - cria indicadores financeiros\n",
    "    - valida consistÃªncia\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"ðŸ”„ Iniciando processamento do SCR.data...\")\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 1. Padronizar nomes das colunas\n",
    "    # ------------------------------------------------\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.lower()\n",
    "        .str.strip()\n",
    "        .str.replace(\" \", \"_\")\n",
    "        .str.replace(\"Ã§\", \"c\")\n",
    "        .str.replace(\"Ã£\", \"a\")\n",
    "        .str.replace(\"Ã¡\", \"a\")\n",
    "        .str.replace(\"Ã©\", \"e\")\n",
    "        .str.replace(\"Ã­\", \"i\")\n",
    "        .str.replace(\"Ã³\", \"o\")\n",
    "        .str.replace(\"Ãº\", \"u\")\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 2. Converter data\n",
    "    # ------------------------------------------------\n",
    "    if \"data_base\" in df.columns:\n",
    "        df[\"data_base\"] = pd.to_datetime(df[\"data_base\"], errors=\"coerce\")\n",
    "\n",
    "        df[\"ano\"] = df[\"data_base\"].dt.year\n",
    "        df[\"mes\"] = df[\"data_base\"].dt.month\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 3. Converter valores monetÃ¡rios\n",
    "    # Detecta automaticamente colunas numÃ©ricas\n",
    "    # ------------------------------------------------\n",
    "    colunas_texto = [\n",
    "        \"uf\", \"segmento\", \"cliente\", \"cnae_ocupacao\",\n",
    "        \"porte\", \"modalidade\", \"submodalidade\",\n",
    "        \"origem\", \"indexador\"\n",
    "    ]\n",
    "\n",
    "    colunas_para_converter = [\n",
    "        c for c in df.columns\n",
    "        if c not in colunas_texto\n",
    "        and c not in [\"data_base\", \"ano\", \"mes\", \"__arquivo_origem\"]\n",
    "    ]\n",
    "\n",
    "    for c in colunas_para_converter:\n",
    "        if df[c].dtype == \"object\":\n",
    "            try:\n",
    "                df[c] = (\n",
    "                    df[c]\n",
    "                    .str.replace(\".\", \"\", regex=False)\n",
    "                    .str.replace(\",\", \".\", regex=False)\n",
    "                )\n",
    "                df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 4. Criar indicadores financeiros\n",
    "    # ------------------------------------------------\n",
    "    if criar_indicadores:\n",
    "\n",
    "        if {\"carteira_inadimplencia\", \"carteira_ativa\"}.issubset(df.columns):\n",
    "            df[\"taxa_inadimplencia\"] = (\n",
    "                df[\"carteira_inadimplencia\"] /\n",
    "                df[\"carteira_ativa\"]\n",
    "            ).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        if {\"carteira_vencida\", \"carteira_ativa\"}.issubset(df.columns):\n",
    "            df[\"perc_carteira_vencida\"] = (\n",
    "                df[\"carteira_vencida\"] /\n",
    "                df[\"carteira_ativa\"]\n",
    "            ).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        if {\"ativo_problematico\", \"carteira_ativa\"}.issubset(df.columns):\n",
    "            df[\"taxa_ativo_problematico\"] = (\n",
    "                df[\"ativo_problematico\"] /\n",
    "                df[\"carteira_ativa\"]\n",
    "            ).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 5. Remover registros sem valor analÃ­tico\n",
    "    # ------------------------------------------------\n",
    "    if remover_zeros and \"carteira_ativa\" in df.columns:\n",
    "        antes = len(df)\n",
    "        df = df[df[\"carteira_ativa\"] > 0]\n",
    "        depois = len(df)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"ðŸ§¹ Removidas {antes - depois:,} linhas com carteira zerada.\")\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 6. Checagem de consistÃªncia\n",
    "    # ------------------------------------------------\n",
    "    if {\"carteira_ativa\", \"carteira_vencida\"}.issubset(df.columns):\n",
    "        inconsistentes = df[df[\"carteira_vencida\"] > df[\"carteira_ativa\"]]\n",
    "\n",
    "        if verbose and len(inconsistentes) > 0:\n",
    "            print(f\"âš ï¸ {len(inconsistentes)} registros inconsistentes encontrados.\")\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 7. Ordenar dataset\n",
    "    # ------------------------------------------------\n",
    "    if \"data_base\" in df.columns:\n",
    "        df = df.sort_values(\"data_base\")\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 8. Reset index\n",
    "    # ------------------------------------------------\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"âœ… SCR.data processado com sucesso!\")\n",
    "        print(f\"Linhas: {len(df):,}\")\n",
    "        print(f\"Colunas: {len(df.columns)}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23acee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_scrdata(\n",
    "    ano: int,\n",
    "    base_dir: Union[str, Path] = \"data/scrdata\",\n",
    "    forcar_download: bool = False,\n",
    "    encoding: str = \"utf-8\",\n",
    "    sep: Optional[str] = None,\n",
    "    salvar_parquet: bool = True,\n",
    "    salvar_csv: bool = False,\n",
    "    adicionar_coluna_origem: bool = True,\n",
    "    remover_zeros: bool = True,\n",
    "    criar_indicadores: bool = True,\n",
    "    verbose: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Pipeline completo do SCR.data:\n",
    "    - baixa ZIP por ano (com cache)\n",
    "    - extrai (com cache)\n",
    "    - concatena todos os CSVs mensais\n",
    "    - processa (tipos, datas, indicadores)\n",
    "    - salva em parquet/csv\n",
    "    - retorna artefatos e DataFrame\n",
    "\n",
    "    Retorna um dicionÃ¡rio com:\n",
    "      - df_raw, df_processed\n",
    "      - paths (zip, extracted_dir, parquet/csv)\n",
    "      - metadata\n",
    "    \"\"\"\n",
    "    if not (1900 <= int(ano) <= 2100):\n",
    "        raise ValueError(f\"Ano invÃ¡lido: {ano}\")\n",
    "\n",
    "    base_dir = Path(base_dir)\n",
    "    ano_dir = base_dir / str(ano)\n",
    "    raw_dir = ano_dir / \"raw\"\n",
    "    extracted_dir = ano_dir / \"extracted\"\n",
    "    processed_dir = ano_dir / \"processed\"\n",
    "\n",
    "    raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "    extracted_dir.mkdir(parents=True, exist_ok=True)\n",
    "    processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    url = f\"https://www.bcb.gov.br/pda/desig/scrdata_{ano}.zip\"\n",
    "    zip_path = raw_dir / f\"scrdata_{ano}.zip\"\n",
    "\n",
    "    # ---------------------------\n",
    "    # 1) Download (cache por ano)\n",
    "    # ---------------------------\n",
    "    if zip_path.exists() and not forcar_download:\n",
    "        if verbose:\n",
    "            print(f\"â„¹ï¸ ZIP {ano} jÃ¡ existe: {zip_path.resolve()}\")\n",
    "            print(\"âž¡ï¸ Download nÃ£o serÃ¡ realizado.\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"â¬‡ï¸ Baixando: {url}\")\n",
    "        r = requests.get(url, timeout=180)\n",
    "        r.raise_for_status()\n",
    "        zip_path.write_bytes(r.content)\n",
    "        if verbose:\n",
    "            print(f\"âœ… ZIP salvo em: {zip_path.resolve()}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2) ExtraÃ§Ã£o (cache)\n",
    "    # ---------------------------\n",
    "    ja_extraido = any(extracted_dir.rglob(\"*.csv\"))\n",
    "    if ja_extraido and not forcar_download:\n",
    "        if verbose:\n",
    "            print(f\"â„¹ï¸ CSVs jÃ¡ extraÃ­dos em: {extracted_dir.resolve()}\")\n",
    "            print(\"âž¡ï¸ ExtraÃ§Ã£o nÃ£o serÃ¡ realizada.\")\n",
    "    else:\n",
    "        if forcar_download:\n",
    "            # limpa extraÃ§Ã£o para evitar mistura\n",
    "            for p in sorted(extracted_dir.rglob(\"*\"), reverse=True):\n",
    "                if p.is_file():\n",
    "                    p.unlink()\n",
    "                elif p.is_dir():\n",
    "                    try:\n",
    "                        p.rmdir()\n",
    "                    except OSError:\n",
    "                        pass\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"ðŸ—œï¸ Extraindo para: {extracted_dir.resolve()}\")\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "            zf.extractall(extracted_dir)\n",
    "        if verbose:\n",
    "            print(\"âœ… ExtraÃ§Ã£o concluÃ­da.\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3) Localizar CSVs\n",
    "    # ---------------------------\n",
    "    csv_paths = sorted(extracted_dir.rglob(\"*.csv\"))\n",
    "    if not csv_paths:\n",
    "        raise FileNotFoundError(f\"Nenhum CSV encontrado em {extracted_dir.resolve()}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"ðŸ“„ CSVs encontrados: {len(csv_paths)}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 4) Ler e concatenar CSVs\n",
    "    # ---------------------------\n",
    "    def inferir_sep(csv_path: Path, enc: str) -> str:\n",
    "        sample = csv_path.read_bytes()[:50_000]\n",
    "        text = sample.decode(enc, errors=\"ignore\")\n",
    "        return \";\" if text.count(\";\") > text.count(\",\") else \",\"\n",
    "\n",
    "    dfs: List[pd.DataFrame] = []\n",
    "    for p in csv_paths:\n",
    "        local_sep = sep or inferir_sep(p, encoding)\n",
    "\n",
    "        try:\n",
    "            df_part = pd.read_csv(p, encoding=encoding, sep=local_sep)\n",
    "        except UnicodeDecodeError:\n",
    "            df_part = pd.read_csv(p, encoding=\"latin1\", sep=local_sep)\n",
    "\n",
    "        if adicionar_coluna_origem:\n",
    "            df_part[\"__arquivo_origem\"] = p.name\n",
    "\n",
    "        dfs.append(df_part)\n",
    "\n",
    "    df_raw = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"âœ… RAW consolidado: {len(df_raw):,} linhas | {len(df_raw.columns)} colunas\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 5) Processar (usa sua funÃ§Ã£o)\n",
    "    # ---------------------------\n",
    "    # Requer que processar_scrdata(df) exista no notebook.\n",
    "    df_processed = processar_scrdata(\n",
    "        df_raw,\n",
    "        remover_zeros=remover_zeros,\n",
    "        criar_indicadores=criar_indicadores,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    # 6) Salvar outputs\n",
    "    # ---------------------------\n",
    "    parquet_path = processed_dir / f\"scrdata_{ano}.parquet\"\n",
    "    csv_path = processed_dir / f\"scrdata_{ano}.csv\"\n",
    "\n",
    "    if salvar_parquet:\n",
    "        df_processed.to_parquet(parquet_path, index=False)\n",
    "        if verbose:\n",
    "            print(f\"ðŸ’¾ Parquet salvo em: {parquet_path.resolve()}\")\n",
    "\n",
    "    if salvar_csv:\n",
    "        df_processed.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "        if verbose:\n",
    "            print(f\"ðŸ’¾ CSV salvo em: {csv_path.resolve()}\")\n",
    "\n",
    "    # Metadados\n",
    "    df_processed.attrs[\"fonte_url\"] = url\n",
    "    df_processed.attrs[\"zip_path\"] = str(zip_path.resolve())\n",
    "    df_processed.attrs[\"extracted_dir\"] = str(extracted_dir.resolve())\n",
    "    df_processed.attrs[\"parquet_path\"] = str(parquet_path.resolve()) if salvar_parquet else None\n",
    "    df_processed.attrs[\"csv_path\"] = str(csv_path.resolve()) if salvar_csv else None\n",
    "    df_processed.attrs[\"csv_count\"] = len(csv_paths)\n",
    "\n",
    "    return {\n",
    "        \"df_raw\": df_raw,\n",
    "        \"df_processed\": df_processed,\n",
    "        \"paths\": {\n",
    "            \"zip\": zip_path.resolve(),\n",
    "            \"extracted_dir\": extracted_dir.resolve(),\n",
    "            \"parquet\": parquet_path.resolve() if salvar_parquet else None,\n",
    "            \"csv\": csv_path.resolve() if salvar_csv else None,\n",
    "        },\n",
    "        \"metadata\": {\n",
    "            \"ano\": ano,\n",
    "            \"url\": url,\n",
    "            \"csv_count\": len(csv_paths),\n",
    "            \"rows_raw\": len(df_raw),\n",
    "            \"rows_processed\": len(df_processed),\n",
    "            \"cols_processed\": len(df_processed.columns),\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d49cd702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    cotacaoCompra  cotacaoVenda         dataHoraCotacao\n",
      "0          5.4366        5.4372 2026-01-02 13:03:33.254\n",
      "1          5.4345        5.4351 2026-01-05 13:10:14.896\n",
      "2          5.3791        5.3797 2026-01-06 13:12:07.358\n",
      "3          5.3874        5.3880 2026-01-07 13:06:27.001\n",
      "4          5.3854        5.3860 2026-01-08 13:04:28.616\n",
      "5          5.3701        5.3707 2026-01-09 13:09:29.054\n",
      "6          5.3754        5.3760 2026-01-12 13:08:29.056\n",
      "7          5.3758        5.3764 2026-01-13 13:08:54.165\n",
      "8          5.3789        5.3795 2026-01-14 13:04:32.411\n",
      "9          5.3840        5.3846 2026-01-15 13:05:25.660\n",
      "10         5.3792        5.3798 2026-01-16 13:04:26.486\n",
      "11         5.3647        5.3653 2026-01-19 13:07:27.662\n",
      "12         5.3784        5.3790 2026-01-20 13:02:30.787\n",
      "13         5.3362        5.3368 2026-01-21 13:04:32.866\n",
      "14         5.3112        5.3118 2026-01-22 13:07:26.401\n",
      "15         5.2872        5.2879 2026-01-23 13:09:25.549\n",
      "16         5.2754        5.2760 2026-01-26 13:09:03.031\n",
      "17         5.2386        5.2392 2026-01-27 13:08:25.783\n",
      "18         5.1832        5.1838 2026-01-28 13:10:27.392\n",
      "19         5.1950        5.1956 2026-01-29 13:08:22.928\n",
      "20         5.2295        5.2301 2026-01-30 13:05:28.730\n",
      "21         5.2581        5.2587 2026-02-02 13:07:27.937\n",
      "22         5.2230        5.2236 2026-02-03 13:11:27.228\n",
      "23         5.2353        5.2359 2026-02-04 13:10:28.563\n",
      "24         5.2574        5.2580 2026-02-05 13:05:29.335\n"
     ]
    }
   ],
   "source": [
    "df_periodo = cotacao_dolar_periodo_df(date(2026, 1, 1), date(2026, 2, 5))\n",
    "print(df_periodo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4502e349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ ZIP 2025 jÃ¡ existe: C:\\Users\\DEINF.LEOBARROS\\Desktop\\Cursos\\PÃ³s\\pos_machine_learning\\Projeto_Final\\data\\scrdata\\2025\\raw\\scrdata_2025.zip\n",
      "âž¡ï¸ Download nÃ£o serÃ¡ realizado.\n",
      "â„¹ï¸ CSVs jÃ¡ extraÃ­dos em: C:\\Users\\DEINF.LEOBARROS\\Desktop\\Cursos\\PÃ³s\\pos_machine_learning\\Projeto_Final\\data\\scrdata\\2025\\extracted\n",
      "âž¡ï¸ ExtraÃ§Ã£o nÃ£o serÃ¡ realizada.\n",
      "ðŸ“„ CSVs encontrados: 10\n",
      "âœ… RAW consolidado: 3,066,086 linhas | 25 colunas\n",
      "ðŸ”„ Iniciando processamento do SCR.data...\n",
      "ðŸ§¹ Removidas 0 linhas com carteira zerada.\n",
      "âœ… SCR.data processado com sucesso!\n",
      "Linhas: 3,066,086\n",
      "Colunas: 30\n",
      "ðŸ’¾ Parquet salvo em: C:\\Users\\DEINF.LEOBARROS\\Desktop\\Cursos\\PÃ³s\\pos_machine_learning\\Projeto_Final\\data\\scrdata\\2025\\processed\\scrdata_2025.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'zip': WindowsPath('C:/Users/DEINF.LEOBARROS/Desktop/Cursos/PÃ³s/pos_machine_learning/Projeto_Final/data/scrdata/2025/raw/scrdata_2025.zip'),\n",
       " 'extracted_dir': WindowsPath('C:/Users/DEINF.LEOBARROS/Desktop/Cursos/PÃ³s/pos_machine_learning/Projeto_Final/data/scrdata/2025/extracted'),\n",
       " 'parquet': WindowsPath('C:/Users/DEINF.LEOBARROS/Desktop/Cursos/PÃ³s/pos_machine_learning/Projeto_Final/data/scrdata/2025/processed/scrdata_2025.parquet'),\n",
       " 'csv': None}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado = pipeline_scrdata(2025, salvar_csv=False, salvar_parquet=True)\n",
    "\n",
    "df_scr_2025 = resultado[\"df_processed\"]\n",
    "resultado[\"paths\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86ddbf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "codigo = \"\"\"\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import date\n",
    "import plotly.graph_objects as go\n",
    "from scr_pipeline import pipeline_scrdata\n",
    "\n",
    "# ==============================\n",
    "# PTAX - DÃ“LAR\n",
    "# ==============================\n",
    "BASE = \"https://olinda.bcb.gov.br/olinda/servico/PTAX/versao/v1/odata\"\n",
    "\n",
    "def _fmt_mmddyyyy(d: date) -> str:\n",
    "    return d.strftime(\"%m-%d-%Y\")\n",
    "\n",
    "@st.cache_data(ttl=3600)\n",
    "def cotacao_dolar_periodo_df(data_ini: date, data_fim: date) -> pd.DataFrame:\n",
    "    url = f\"{BASE}/CotacaoDolarPeriodo(dataInicial=@dataInicial,dataFinalCotacao=@dataFinalCotacao)\"\n",
    "    params = {\n",
    "        \"@dataInicial\": f\"'{_fmt_mmddyyyy(data_ini)}'\",\n",
    "        \"@dataFinalCotacao\": f\"'{_fmt_mmddyyyy(data_fim)}'\",\n",
    "        \"$format\": \"json\",\n",
    "        \"$select\": \"cotacaoCompra,cotacaoVenda,dataHoraCotacao\",\n",
    "        \"$top\": 10000,\n",
    "    }\n",
    "\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    if not r.ok:\n",
    "        raise requests.HTTPError(f\"{r.status_code} - {r.text}\", response=r)\n",
    "\n",
    "    payload = r.json()\n",
    "    df = pd.DataFrame(payload.get(\"value\", []))\n",
    "\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df[\"dataHoraCotacao\"] = pd.to_datetime(df[\"dataHoraCotacao\"])\n",
    "    df = df.sort_values(\"dataHoraCotacao\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def dolar_diario(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    df[\"data\"] = df[\"dataHoraCotacao\"].dt.date\n",
    "    out = (\n",
    "        df.groupby(\"data\", as_index=False)[[\"cotacaoCompra\", \"cotacaoVenda\"]]\n",
    "          .mean()\n",
    "          .rename(columns={\n",
    "              \"cotacaoCompra\": \"compra\",\n",
    "              \"cotacaoVenda\": \"venda\"\n",
    "          })\n",
    "    )\n",
    "    out[\"data\"] = pd.to_datetime(out[\"data\"])\n",
    "    return out.sort_values(\"data\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# SCR\n",
    "# ==============================\n",
    "@st.cache_data(show_spinner=True)\n",
    "def carregar_scr(ano: int) -> pd.DataFrame:\n",
    "    resultado = pipeline_scrdata(\n",
    "        ano,\n",
    "        salvar_parquet=True,\n",
    "        salvar_csv=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    return resultado[\"df_processed\"]\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# UI\n",
    "# ==============================\n",
    "st.set_page_config(\n",
    "    page_title=\"Painel BCB - DÃ³lar e SCR\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "st.title(\"ðŸ“Š Painel BCB â€” DÃ³lar (PTAX) e SCR.data\")\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header(\"Fonte de dados\")\n",
    "    fonte = st.selectbox(\n",
    "        \"Escolha o dado\",\n",
    "        [\"DÃ³lar comercial (PTAX)\", \"SCR.data\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# DÃ“LAR\n",
    "# ==============================\n",
    "if fonte == \"DÃ³lar comercial (PTAX)\":\n",
    "\n",
    "    st.subheader(\"ðŸ’µ DÃ³lar comercial â€” PTAX\")\n",
    "\n",
    "    hoje = date.today()\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.header(\"ParÃ¢metros\")\n",
    "        data_ini = st.date_input(\"Data inicial\", value=date(hoje.year, 1, 1))\n",
    "        data_fim = st.date_input(\"Data final\", value=hoje)\n",
    "        consultar = st.button(\"Consultar\")\n",
    "\n",
    "    if consultar:\n",
    "        if data_ini > data_fim:\n",
    "            st.error(\"A data inicial nÃ£o pode ser maior que a data final.\")\n",
    "            st.stop()\n",
    "\n",
    "        with st.spinner(\"Consultando API do Banco Central...\"):\n",
    "            df_raw = cotacao_dolar_periodo_df(data_ini, data_fim)\n",
    "            df = dolar_diario(df_raw)\n",
    "\n",
    "        if df.empty:\n",
    "            st.warning(\"Nenhuma cotaÃ§Ã£o encontrada para o perÃ­odo.\")\n",
    "            st.stop()\n",
    "\n",
    "        ultima = df.iloc[-1]\n",
    "        c1, c2 = st.columns(2)\n",
    "        c1.metric(\"Ãšltima compra\", f\"R$ {ultima['compra']:.4f}\")\n",
    "        c2.metric(\"Ãšltima venda\", f\"R$ {ultima['venda']:.4f}\")\n",
    "\n",
    "        st.subheader(\"ðŸ“ˆ EvoluÃ§Ã£o diÃ¡ria do dÃ³lar\")\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=df[\"data\"], y=df[\"compra\"], name=\"Compra\"))\n",
    "        fig.add_trace(go.Scatter(x=df[\"data\"], y=df[\"venda\"], name=\"Venda\"))\n",
    "        fig.update_layout(\n",
    "            height=520,\n",
    "            xaxis_title=\"Data\",\n",
    "            yaxis_title=\"R$\",\n",
    "            margin=dict(l=10, r=10, t=40, b=10)\n",
    "        )\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "        st.dataframe(df, use_container_width=True)\n",
    "\n",
    "        st.download_button(\n",
    "            \"â¬‡ï¸ Baixar CSV\",\n",
    "            data=df.to_csv(index=False).encode(\"utf-8\"),\n",
    "            file_name=\"cotacao_dolar_ptax.csv\",\n",
    "            mime=\"text/csv\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        st.info(\"Selecione o perÃ­odo e clique em **Consultar**.\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# SCR\n",
    "# ==============================\n",
    "else:\n",
    "    st.subheader(\"ðŸ“Š SCR.data â€” Sistema de InformaÃ§Ãµes de CrÃ©dito\")\n",
    "\n",
    "    with st.sidebar:\n",
    "        ano = st.number_input(\"Ano\", min_value=2010, max_value=2100, value=2025, step=1)\n",
    "\n",
    "    df = carregar_scr(int(ano))\n",
    "\n",
    "    if df.empty:\n",
    "        st.warning(\"Nenhum dado disponÃ­vel para o ano selecionado.\")\n",
    "        st.stop()\n",
    "\n",
    "    df[\"data_base\"] = pd.to_datetime(df[\"data_base\"])\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.header(\"Filtros\")\n",
    "        uf = st.selectbox(\"UF\", [\"(Todas)\"] + sorted(df[\"uf\"].dropna().unique()))\n",
    "        cliente = st.selectbox(\"Cliente\", [\"(Todos)\"] + sorted(df[\"cliente\"].dropna().unique()))\n",
    "        modalidade = st.selectbox(\"Modalidade\", [\"(Todas)\"] + sorted(df[\"modalidade\"].dropna().unique()))\n",
    "\n",
    "    df_f = df.copy()\n",
    "    if uf != \"(Todas)\": df_f = df_f[df_f[\"uf\"] == uf]\n",
    "    if cliente != \"(Todos)\": df_f = df_f[df_f[\"cliente\"] == cliente]\n",
    "    if modalidade != \"(Todas)\": df_f = df_f[df_f[\"modalidade\"] == modalidade]\n",
    "\n",
    "    st.caption(f\"Registros apÃ³s filtros: {len(df_f):,}\")\n",
    "\n",
    "    c1, c2, c3 = st.columns(3)\n",
    "    c1.metric(\"Carteira ativa\", f\"{df_f['carteira_ativa'].sum():,.2f}\")\n",
    "    c2.metric(\"InadimplÃªncia\", f\"{df_f['carteira_inadimplencia'].sum():,.2f}\")\n",
    "\n",
    "    taxa = (\n",
    "        (df_f[\"taxa_inadimplencia\"] * df_f[\"carteira_ativa\"]).sum()\n",
    "        / df_f[\"carteira_ativa\"].sum()\n",
    "        if df_f[\"carteira_ativa\"].sum() > 0 else 0\n",
    "    )\n",
    "    c3.metric(\"Taxa inadimplÃªncia\", f\"{taxa:.2%}\")\n",
    "\n",
    "    st.subheader(\"ðŸ“ˆ EvoluÃ§Ã£o da carteira ativa\")\n",
    "    serie_ativa = df_f.groupby(\"data_base\")[\"carteira_ativa\"].sum()\n",
    "    st.line_chart(serie_ativa, use_container_width=True)\n",
    "\n",
    "    st.subheader(\"ðŸ“ˆ EvoluÃ§Ã£o da inadimplÃªncia\")\n",
    "    serie_inad = df_f.groupby(\"data_base\")[\"carteira_inadimplencia\"].sum()\n",
    "    st.line_chart(serie_inad, use_container_width=True)\n",
    "\n",
    "    st.subheader(\"ðŸ“Š Top modalidades por carteira ativa\")\n",
    "    top_mod = df_f.groupby(\"modalidade\")[\"carteira_ativa\"].sum().sort_values(ascending=False).head(15)\n",
    "    st.bar_chart(top_mod, use_container_width=True)\n",
    "\n",
    "    st.subheader(\"ðŸ“Š Top UFs por carteira ativa\")\n",
    "    top_uf = df_f.groupby(\"uf\")[\"carteira_ativa\"].sum().sort_values(ascending=False).head(15)\n",
    "    st.bar_chart(top_uf, use_container_width=True)\n",
    "\n",
    "    st.subheader(\"ðŸ“¥ Exportar dados filtrados\")\n",
    "    st.download_button(\n",
    "        \"Baixar CSV\",\n",
    "        data=df_f.to_csv(index=False).encode(\"utf-8\"),\n",
    "        file_name=f\"scr_filtrado_{ano}.csv\",\n",
    "        mime=\"text/csv\"\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "db243a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… app.py criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "with open(\"app.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(codigo)\n",
    "\n",
    "print(\"âœ… app.py criado com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
